{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports and dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "num_split = int(0.7 * len(digits.data))\n",
    "train_features = digits.data[:num_split]  # train data (features)\n",
    "train_labels = digits.target[:num_split]  # train labels (y-values)\n",
    "test_features = digits.data[num_split:]  # test data (features)\n",
    "test_labels = digits.target[num_split:]  # test labels(y-values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the train dataset to values between 0 and 1\n",
    "for i in range(len(train_features)):\n",
    "    for j in range(len(train_features[i])):\n",
    "        train_features[i][j] /=16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EM-algorithm to find a Gaussian NBC\n",
    "- Assume conditional independence (covariance = variance for the attribute)\n",
    "- Normalize the data to avoid multiplications with very small values in the likelihoods\n",
    "- You can use the overall change in cluster centers between two iterations as a stop criterion. Minimal movement: assume convergence\n",
    "- Make sure dimensions of covariance matrix are correct, possible to only calculate the diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EM_:\n",
    "    \n",
    "    def __init__(self, X, K):\n",
    "        \n",
    "        # Initialize prior probs, means and covariances\n",
    "        self.prior = np.ones(K)/K\n",
    "        self.means = np.zeros([K, len(X[0])])\n",
    "        self.cov = np.ones([K, len(X[0])])*0.1\n",
    "        self.r = np.zeros([len(X), K])\n",
    "\n",
    "        # Creating subsets \n",
    "        subset = []\n",
    "        start = 0\n",
    "        end = int(len(X))/K\n",
    "        for i in range(K):\n",
    "            subset.append(X[start:int((i+1)*end)])\n",
    "            start = int((i+1)*end)\n",
    "        \n",
    "        # Calculating first means \n",
    "        for k,subs in enumerate(subset):\n",
    "            N = len(subs)\n",
    "            for j, image in enumerate(subs):\n",
    "                for h, pixel in enumerate(image):\n",
    "                    self.means[k][h] +=pixel/N \n",
    "       \n",
    "        # Initiating covariances (variances here)\n",
    "        for i,subs in enumerate(subset):\n",
    "            N = len(subs)\n",
    "            for j, image in enumerate(subs):\n",
    "                for h, pixel in enumerate(image):\n",
    "                    self.cov[i][h] += ((pixel-self.means[i][h])**2)/N\n",
    "        \n",
    "    def fit(self, X, K):\n",
    "        diff = 10\n",
    "        nr_it = 0\n",
    "        mean_last_step = self.means.copy()\n",
    "        \n",
    "        while diff>0.001: \n",
    "            nr_it +=1\n",
    "                \n",
    "            # E-step\n",
    "            EM_.E_step_(self,X,K)\n",
    "                \n",
    "            # M-step\n",
    "            EM_.M_step_(self,X, K)\n",
    "                \n",
    "            diff  = np.linalg.norm(mean_last_step-self.means)\n",
    "            mean_last_step = self.means.copy()\n",
    "            print('Iteration: ' , nr_it, 'Diff: ', diff)\n",
    "                  \n",
    "        return         \n",
    "        \n",
    "    # Expectation step\n",
    "    def E_step_(self,X,K):\n",
    "        cov = self.cov\n",
    "        means = self.means\n",
    "        prior = self.prior\n",
    "        self.r = np.zeros([len(X), K])\n",
    "\n",
    "        # Iterating through all images and for each image we iterate through all classes and pixels\n",
    "        for i,image in enumerate(X):\n",
    "            numerator = np.zeros(K)\n",
    "            denominator = 0\n",
    "            \n",
    "            for k in range(K):\n",
    "                prob = 1\n",
    "                \n",
    "                # Calculating and multiplying the gaussian probabilistic term for the pixel\n",
    "                for p, pixel in enumerate(image):\n",
    "                    prob *= 1/(np.sqrt(2*np.pi*cov[k][p])) * np.exp(-(pixel-means[k][p])**2/(2*cov[k][p]))\n",
    "                            \n",
    "                numerator[k] = prior[k]*prob\n",
    "                denominator += prior[k]*prob\n",
    "            \n",
    "            # r consists of a list in which every element represents an image by a list of the corresponding probabilities for each class\n",
    "            self.r[i] = numerator/denominator \n",
    "            \n",
    "        return\n",
    "        \n",
    "    def M_step_(self, X, K):\n",
    "            \n",
    "        # Computing r_k: the sum of all probabilities for each class, has dim 10\n",
    "        r_k = np.zeros([K])\n",
    "        for i, image in enumerate(self.r):\n",
    "            for k, prob in enumerate(image):\n",
    "                r_k[k] += prob\n",
    "    \n",
    "        # Computing new prior probabilities\n",
    "        self.prior = r_k / len(X)\n",
    "            \n",
    "        # Updating means and covariances\n",
    "        for k in range(K):\n",
    "            mean_k = 0\n",
    "            cov_k = 0\n",
    "                \n",
    "            for i, image in enumerate(X):\n",
    "                cov_k += self.r[i][k]*(image*np.transpose(image))   \n",
    "                mean_k += self.r[i][k] * image\n",
    "            self.means[k] = mean_k/r_k[k]  \n",
    "            self.cov[k] = cov_k/r_k[k] - mean_k/r_k[k]*np.transpose(mean_k/r_k[k]) + 0.01   \n",
    "\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def predict_(self,X):\n",
    "        predictions = []\n",
    "        prior = self.prior\n",
    "        cov = self.cov\n",
    "        means = self.means\n",
    "        \n",
    "        # Iterating through all images\n",
    "        for image in X:\n",
    "            probability_values = np.zeros(len(prior))\n",
    "        \n",
    "            # Iterating through all possible classes with their prior probabilities and multiply probabilities corresponding to the class \n",
    "            for k,prior_prob in enumerate(prior):\n",
    "                probability_values[k] = prior_prob\n",
    "            \n",
    "                # Iterating through the image's pixels\n",
    "                for p, pixel in enumerate(image):\n",
    "                    probability_values[k] *= 1/(np.sqrt(2*np.pi*cov[k][p])) * np.exp(-(pixel-means[k][p])**2/(2*cov[k][p]))\n",
    "    \n",
    "            # Appending the class with highest probability\n",
    "            predictions.append(np.argmax(probability_values))\n",
    "\n",
    "        return predictions\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "EM = EM_(train_features, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1 Diff:  1.210593924688377\n",
      "Iteration:  2 Diff:  1.7719440934318422\n",
      "Iteration:  3 Diff:  0.9828110096261158\n",
      "Iteration:  4 Diff:  0.6963874001363991\n",
      "Iteration:  5 Diff:  0.6033863352751674\n",
      "Iteration:  6 Diff:  0.595243587094797\n",
      "Iteration:  7 Diff:  0.5057529789511146\n",
      "Iteration:  8 Diff:  0.29200166890061213\n",
      "Iteration:  9 Diff:  0.25497934411289463\n",
      "Iteration:  10 Diff:  0.24553961456549103\n",
      "Iteration:  11 Diff:  0.16175876647675494\n",
      "Iteration:  12 Diff:  0.12538869202540928\n",
      "Iteration:  13 Diff:  0.138604099854422\n",
      "Iteration:  14 Diff:  0.11986529155183093\n",
      "Iteration:  15 Diff:  0.08816107610681838\n",
      "Iteration:  16 Diff:  0.06237092016726197\n",
      "Iteration:  17 Diff:  0.0519778059873326\n",
      "Iteration:  18 Diff:  0.03441388246594154\n",
      "Iteration:  19 Diff:  0.03235733810680268\n",
      "Iteration:  20 Diff:  0.01822314976005127\n",
      "Iteration:  21 Diff:  0.012603391239107878\n",
      "Iteration:  22 Diff:  0.01171935367481656\n",
      "Iteration:  23 Diff:  0.009199813546660088\n",
      "Iteration:  24 Diff:  0.005768195013525328\n",
      "Iteration:  25 Diff:  0.004827837957315897\n",
      "Iteration:  26 Diff:  0.006013861968576382\n",
      "Iteration:  27 Diff:  0.008347609516530064\n",
      "Iteration:  28 Diff:  0.0069152085801107216\n",
      "Iteration:  29 Diff:  0.0033363963259906383\n",
      "Iteration:  30 Diff:  0.0014674000947788827\n",
      "Iteration:  31 Diff:  0.0008719198572827605\n"
     ]
    }
   ],
   "source": [
    "EM.fit(train_features,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = EM.predict_(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = KMeans(n_clusters=10)\n",
    "clusters = KM.fit(train_features)\n",
    "predictions_KMeans = KM.predict(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Completeness score: all members of a given class are assigned to the same cluster.\n",
    "- Homogeneity score: each cluster contains only members of a single class\n",
    "- Mutual information score: how much does one cluster say about the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans clustering\n",
      "Confusion matrix:\n",
      " [[  0   0   0   0   0   0   0 125   0   0]\n",
      " [  0   0  63   1  39   0   0   0   0  26]\n",
      " [  0   3  10   0   3   3   0   0   1 104]\n",
      " [  0 113   0   0   0   2   2   0  13   0]\n",
      " [109   0   2   0   6   7   0   0   0   0]\n",
      " [  1   2   0   1   0   0  93   0  29   0]\n",
      " [  0   0   2 124   0   0   0   1   0   0]\n",
      " [  0   0   0   0   2 123   0   0   0   0]\n",
      " [  0   2  70   1   4   1   4   0  38   2]\n",
      " [  0   3   2   0  15   6   2   0  97   0]]\n",
      "Completeness score:  0.7564104368068433\n",
      "Homogeneity score:  0.7481511041008959\n",
      "Mutual information score:  0.7486554642198753\n"
     ]
    }
   ],
   "source": [
    "print('KMeans clustering\\nConfusion matrix:\\n',metrics.confusion_matrix(train_labels,predictions_KMeans))\n",
    "print('Completeness score: ',metrics.completeness_score( train_labels, predictions_KMeans))\n",
    "print('Homogeneity score: ',metrics.homogeneity_score( train_labels, predictions_KMeans))\n",
    "print('Mutual information score: ',metrics.adjusted_mutual_info_score( train_labels, predictions_KMeans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM algorithm \n",
      "Confusion matrix:\n",
      " [[  1   0   0 124   0   0   0   0   0   0]\n",
      " [ 41  60   0   0   0   0   0  27   0   1]\n",
      " [  6   0  19   0   0   0   0  99   0   0]\n",
      " [  0   0  32   0   0   0   2   2  94   0]\n",
      " [ 67   0   0   0   0  48   8   0   0   1]\n",
      " [  0   0   1   0   0   2   0   0  28  95]\n",
      " [  1   1   0   0 125   0   0   0   0   0]\n",
      " [  0   0   0   0   0  24 101   0   0   0]\n",
      " [  2  10 100   0   0   0   0   4   2   4]\n",
      " [  2   0   8   0   0  16   4   0  95   0]]\n",
      "Completeness score:  0.7273059589167915\n",
      "Homogeneity score:  0.71335151689085\n",
      "Mutual information score:  0.7161737273016219\n"
     ]
    }
   ],
   "source": [
    "print('EM algorithm \\nConfusion matrix:\\n',metrics.confusion_matrix(train_labels,predictions))\n",
    "print('Completeness score: ',metrics.completeness_score( train_labels, predictions))\n",
    "print('Homogeneity score: ',metrics.homogeneity_score( train_labels, predictions))\n",
    "print('Mutual information score: ',metrics.adjusted_mutual_info_score( train_labels, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
